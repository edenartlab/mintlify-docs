---
title: Using SDXL LoRA's locally  
description: Complete guide to Eden's creation endpoints for images and videos
---

The easiest way to make creations with Eden is through the [creation tool frontend](https://app.eden.art/create/creations).

<Tip>
You can also interact with the creator tool [through the SDK](/guides/sdk).
</Tip>

## Overview

Eden offers a number of generative pipelines for making images and videos, mostly built on top of the [Stable Diffusion](https://stability.ai/stablediffusion) model family. The pipelines are divided into a number of *endpoints* or *generators* (terms used interchangeably) which are optimized for different visual tasks.

## Summary of endpoints

<CardGroup cols={2}>
  <Card title="Image endpoints" icon="image">
    - **/create** - General-purpose text-to-image
    - **/remix** - Generate variations of an uploaded image
    - **/blend** - Mix two uploaded images
    - **/controlnet** - Prompt-guided style transfer
    - **/upscale** - Upscale images to higher resolution
  </Card>
  
  <Card title="Video endpoints" icon="video">
    - **/interpolate** - Gradual interpolation through prompts
    - **/real2real** - Interpolation through uploaded images
    - **/img2vid** - Animate a starting image
    - **/txt2vid** - Generate video from prompts
    - **/vid2vid** - Apply style transfer to video
  </Card>
</CardGroup>

## Image Endpoints

### /create

**[Create](https://app.eden.art/create/creations)** is our default *text-to-image* endpoint. Simply enter a prompt, click "Create" and wait a few moments for the resulting image.

<img src="/img/create.jpg" alt="Creation tool interface" />

Besides the prompt, you are able to request 1, 2, or 4 different samples.

<Accordion title="Settings">
  <AccordionGroup>
    <Accordion title="Common settings">
      - **Width** and **Height**: Set the resolution of the image
      
      <Note>
      Because SDXL was trained at 1024x1024, increasing the resolution beyond that often causes visual artifacts. If you want large images, generate at normal resolution first, then upscale.
      </Note>
    </Accordion>
    
    <Accordion title="Advanced settings">
      - **Negative prompt**: Specify what you *don't* want to see
      - **Prompt strength**: Controls how strongly the prompt drives creation
      - **Sampler**: Sets the diffusion sampler to use
      - **Steps**: Number of denoising steps (30+ has diminishing returns)
      - **Seed**: Random seed for reproducibility
    </Accordion>
  </AccordionGroup>
</Accordion>

<Accordion title="Starting Image">
  Use an uploaded image as a starting point for the creation. The starting image will constrain the final creation to resemble its shape and form.
  
  - **Starting image strength**: Controls influence (0.2 is medium, 0.5+ looks almost identical)
  - **Adopt aspect ratio**: Match the aspect ratio of the starting image
  
  <img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/init_img.jpg" alt="Example with starting image" />
</Accordion>

### /remix

Takes an input image and creates variations using [IP adapter](https://ip-adapter.github.io/) and prompt reconstruction.

<img src="/img/generators/remix.jpg" alt="Remix example" />

**Additional settings:**
- **Image strength**: Influence of starting image (0.0 = regenerate content, higher = closer to original)
- **Remix prompt**: Optional prompt to guide generation
- **Image prompt strength**: Balance between image and prompt influence
- **Upscale Factor**: Upscale output resolution

### /blend

Similar to /remix but takes two input images and creates a novel mixture.

<img src="/img/generators/blend.jpg" alt="Blend example" />

**Specific settings:**
- **Override prompts**: Custom prompts for each input
- **Interpolation seeds**: Seeds for reproducibility
- **Image strengths**: Strength of each image (0.0-0.1 recommended)

### /controlnet

[Controlnet](https://arxiv.org/abs/2302.05543) guides image generation with spatial conditioning from a control image.

<img src="/img/generators/controlnet.jpg" alt="Controlnet example" />

**Parameters:**
- **Shape guidance image**: Control image for spatial guidance
- **Shape guidance image strength**: Usually above 0.5
- **Controlnet mode**:
  - "canny-edge": Match edges and lines
  - "depth": Match perceived depth
  - "luminance": Mimic bright/dark regions

<img src="/img/generators/controlnet_style_transfer.jpg" alt="Style transfer example" />

### /upscale

Produces an upscaled version of an input image.

<img src="/img/generators/upscale.jpg" alt="Upscaling example" />

**Parameters:**
- **Init Image**: Image to upscale
- **AI creativity**: 0.2-0.4 for similarity, 0.4-0.7 for generated details
- **Width/Height**: Maximum dimensions (maintains aspect ratio)

## Video Endpoints

### /interpolate

Generates smooth videos interpolating through text prompts.

<video controls>
  <source src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/tree_lerp.mp4" />
</video>

**Required:**
- **Prompts**: Array of prompts to interpolate through

**Optional:**
- **Width/Height**: Video resolution
- **Frames**: Number of frames
- **Loop**: Loop back to first prompt
- **Smooth**: Apply smoothing algorithm
- **FILM Iterations**: Double frames using FILM
- **FPS**: Frames per second

<Accordion title="Controlnet support">
  - **Shape Guidance Image**: Control image for entire video
  - **Shape guidance strength**: Usually above 0.5
  - **Controlnet mode**: off/canny-edge/depth/luminance
</Accordion>

### /real2real

Generates videos interpolating through uploaded images (keyframes).

<img src="https://storage.googleapis.com/public-assets-xander/A_workbox/eden_docs/real2real_input.jpg" alt="Real2real keyframes" />

**Additional parameters:**
- **Override prompts**: Custom prompts for each keyframe
- **Fading smoothness**: Low = rich journey, high = alpha-fading
- **Keyframe strength**: 1.0 = exact reproduction at keyframes

### /txt2vid

Turns prompts into video animations using actual video generation models.

**Parameters:**
- **Width/Height**: Pixel dimensions
- **Number of frames**: Final video = n_frames / 8 seconds
- **Loop**: Generate seamless loop
- **Motion Scale**: 0.8 = subtle, 1.1 = default, 1.25 = heavy

### /img2vid

Animates a single input image using [AnimateDiff](https://github.com/guoyww/AnimateDiff).

**Parameters:**
- **AI Strength**: Diffusion amount for animation
- **Animation prompt**: Optional motion prompt
- **Motion Scale**: Amount of motion
- **Motion Brush Mask**: Optional mask for selective animation

### /vid2vid

Recreates input video in the style of provided images.

**Parameters:**
- **Shape guidance method**: Coarse or fine guidance
- **Optional style prompt**: Additional style description
- **AI Strength**: Diffusion amount (0.95 default)
- **Shape Control Strength**: Shape influence
- **Motion Scale**: Motion amount

## Creating with concepts

<Tip>
To train your own concept, see [training concepts](/guides/concepts).
</Tip>

Concepts are custom objects, styles, or people trained using LoRA technique. They're available in all creation endpoints.

### Using concepts

1. Click "Select Concept" in the creation interface
2. Toggle between "All Concepts" and "My Concepts"
3. Reference in prompts using the concept name or `<concept>`

<img src="/img/conceptselector.jpg" alt="Concept selector" />

### Composing with concepts

Reference the concept in your prompt (not case-sensitive):
- A photograph of Alice training a neural network
- A photograph of alice training a neural network (using angle brackets)
- A photograph of concept training a neural network (using angle brackets)

<Tip>
- **Face/Object mode**: Reference the concept in your prompt
- **Style mode**: Concept is automatically triggered
</Tip>

### Adjusting concept strength

The "Concept strength" parameter controls influence. Increase if creations don't resemble the concept enough, decrease if the prompt is being ignored.